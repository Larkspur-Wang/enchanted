{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85de34ef-8404-44c8-93d2-2934e3ea2460",
   "metadata": {},
   "source": [
    "步骤 1 - 在 Google Colab 上运行 Ollama\n",
    "开始打开一个新的 Google Colab 笔记本。为了确保我们正在使用 GPU，点击菜单中的“运行时”，选择“更改运行时类型”，并选择“T4 GPU”作为硬件加速器。\n",
    "现在，我们需要安装与 Ngrok 相关的依赖项，并配置 authtoken。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9f410a-b20f-49c8-b54b-901f77885b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install aiohttp pyngrok                \n",
    "!ngrok config add-authtoken [2jGwvYUFzOlOqXi4LtZhEz5br7W_6FL4uUfUqsT942i4xTvE7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2f83af-ef21-427e-afd3-b5a1e755e2a7",
   "metadata": {},
   "source": [
    "使用以下命令安装 Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e908a02a-7531-4209-bfc9-85dc8621ebca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://ollama.ai/install.sh |sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf705d4-dad0-4036-936c-0596fe977943",
   "metadata": {},
   "source": [
    "现在，是时候在 NGrok 旁边运行 Ollama 了。在这个例子中，我们将配置 Ollama 同时运行 Mistral 和 Codellama 模型。这个设置允许我们以后在不同模型之间切换，为我们的聊天应用程序的功能提供了灵活性。\n",
    "为了管理这些，我们将它们作为单独的子进程运行：\n",
    "•Ollama 将启动模型。localhost:11434。\n",
    "•NGrok 然后会暴露端口11434连接到互联网使用公共 URL。\n",
    "执行以下代码片段以启动 Ollama 和 NGrok："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4d734c-a2a9-4224-9543-957acdadc77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                \n",
    "import asyncio                \n",
    "\n",
    "# Set LD_LIBRARY_PATH so the system NVIDIA library                \n",
    "os.environ.update({'LD_LIBRARY_PATH': '/usr/lib64-nvidia'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ac58e3-8672-44e2-8082-ac55ca71c7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "asyncdef run_process(cmd):                  \n",
    "  print('>>> starting', *cmd)                  \n",
    "  p = await asyncio.subprocess.create_subprocess_exec(     \n",
    "      *cmd,\n",
    "      stdout=asyncio.subprocess.PIPE,                      \n",
    "      stderr=asyncio.subprocess.PIPE,                  \n",
    "  )                \n",
    "\n",
    "asyncdef pipe(lines):                    \n",
    "      asyncfor line in lines:                      \n",
    "      print(line.strip().decode('utf-8'))                \n",
    "\n",
    "\n",
    "await asyncio.gather(                      \n",
    "      pipe(p.stdout),                      \n",
    "      pipe(p.stderr),                  \n",
    ")                \n",
    "\n",
    "\n",
    "await asyncio.gather(                    \n",
    "    run_process(['ollama', 'serve']),                    \n",
    "    run_process(['ollama', 'pull', 'gemma2']),                    \n",
    "    run_process(['ngrok', 'http', '--log', 'stderr', '11434']),    \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
